{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cfa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import coup\n",
    "from coup import CoupGame\n",
    "from tqdm import tqdm\n",
    "\n",
    "import enum\n",
    "import random\n",
    "import pyspiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b354fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoupGamePlayground:\n",
    "    def __init__(self, game, solver, show_ai_suggestion=False):\n",
    "        self.game = game\n",
    "        self.solver = solver\n",
    "        self.average_strategy_p0, self.average_strategy_p1 = solver.get_average_strategy()\n",
    "        self.action_names = {\n",
    "            0: \"INCOME\",\n",
    "            1: \"AID\",\n",
    "            2: \"COUP\",\n",
    "            3: \"TAX\",\n",
    "            4: \"ASSASSINATE\",\n",
    "            5: \"STEAL\",\n",
    "            6: \"BLOCKSTEAL\",\n",
    "            7: \"BLOCKAID\",\n",
    "            8: \"BLOCKASS\",\n",
    "            9: \"CHALLENGE\",\n",
    "            10: \"SWAP1\",\n",
    "            11: \"SWAP2\",\n",
    "            12: \"FOLD1\",\n",
    "            13: \"FOLD2\",\n",
    "            14: \"NOCHALL\",\n",
    "        }\n",
    "        self.show_ai_suggestion = show_ai_suggestion  # Toggle for AI suggestions\n",
    "\n",
    "    def play(self):\n",
    "        state = self.game.new_initial_state()\n",
    "\n",
    "        # Allow the human player to choose their player number\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"Do you want to play as Player 0 or Player 1? Enter 0 or 1: \")\n",
    "                human_player = int(user_input)\n",
    "                if human_player in [0, 1]:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid choice. Please enter 0 or 1.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter 0 or 1.\")\n",
    "\n",
    "        ai_player = 1 - human_player\n",
    "\n",
    "        print(f\"\\nWelcome to Coup! You are Player {human_player}.\")\n",
    "        print(\"Game start!\")\n",
    "\n",
    "        while not state.is_terminal():\n",
    "            \n",
    "            current_player = state.current_player()\n",
    "            \n",
    "            print(\"------------------------------------------------------\")\n",
    "            print(\"Current Action count: \", state.action_count)\n",
    "            print(\"Player: \", state.current_player())\n",
    "            \n",
    "\n",
    "            if current_player == human_player:\n",
    "                # Display game state\n",
    "                \n",
    "                print(\"\\nYour turn:\")\n",
    "                self.display_state(state, human_player)\n",
    "\n",
    "                # Get legal actions\n",
    "                legal_actions = state.legal_actions(human_player)\n",
    "                print(\"Legal actions:\")\n",
    "                for action in legal_actions:\n",
    "                    print(f\"{action}: {self.action_names[action]}\")\n",
    "\n",
    "                # Show AI's suggested action distribution if the feature is enabled\n",
    "                if self.show_ai_suggestion:\n",
    "                    action_distribution = self.get_ai_action_distribution(state, human_player, legal_actions)\n",
    "                    print(\"\\nAI suggests the following action probabilities based on the trained strategy:\")\n",
    "                    for action, prob in action_distribution.items():\n",
    "                        action_name = self.action_names[action]\n",
    "                        print(f\"  {action}: {action_name} - Probability: {prob:.2f}\")\n",
    "\n",
    "                # Prompt user for action\n",
    "                while True:\n",
    "                    try:\n",
    "                        user_input = input(\"Enter the action number you want to take: \")\n",
    "                        action = int(user_input)\n",
    "                        if action in legal_actions:\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid action. Please choose a legal action.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid input. Please enter a number corresponding to a legal action.\")\n",
    "\n",
    "                # Apply the action\n",
    "                state.apply_action(action)\n",
    "\n",
    "            elif current_player == ai_player:\n",
    "                # AI's turn\n",
    "                print(\"\\nAI's turn:\")\n",
    "                action = self.get_ai_action(state, ai_player)\n",
    "                print(f\"AI chooses action: {self.action_names[action]}\")\n",
    "                # Apply the action\n",
    "                state.apply_action(action)\n",
    "            else:\n",
    "                # Chance node (e.g., card drawing)\n",
    "                outcomes = state.chance_outcomes()\n",
    "                actions, probs = zip(*outcomes)\n",
    "                action = np.random.choice(actions, p=probs)\n",
    "                state.apply_action(action)\n",
    "\n",
    "        # Game is over\n",
    "        returns = state.returns()\n",
    "        print(\"\\nGame over.\")\n",
    "        print(\"original cards: \", state.original_cards)\n",
    "        \n",
    "        if returns[human_player] > returns[ai_player]:\n",
    "            print(\"You win!\")\n",
    "        else:\n",
    "            print(\"You lose.\")\n",
    "        self.display_state(state, human_player)\n",
    "\n",
    "    def get_ai_action_distribution(self, state, player, legal_actions):\n",
    "        \"\"\"\n",
    "        Returns the AI's action distribution for the human player's turn.\n",
    "        \"\"\"\n",
    "        infoset = state.information_state_string(player)\n",
    "        strategy = self.get_average_strategy_for_player(player).get(infoset)\n",
    "\n",
    "        if strategy is not None:\n",
    "            # Extract probabilities for legal actions\n",
    "            action_probs = {action: strategy[action] for action in legal_actions}\n",
    "            # Normalize the probabilities\n",
    "            total_prob = sum(action_probs.values())\n",
    "            if total_prob > 0:\n",
    "                action_probs = {action: prob / total_prob for action, prob in action_probs.items()}\n",
    "            else:\n",
    "                num_actions = len(legal_actions)\n",
    "                action_probs = {action: 1.0 / num_actions for action in legal_actions}\n",
    "        else:\n",
    "            # Default to uniform probabilities if no strategy is available\n",
    "            num_actions = len(legal_actions)\n",
    "            action_probs = {action: 1.0 / num_actions for action in legal_actions}\n",
    "\n",
    "        return action_probs\n",
    "\n",
    "    def get_ai_action(self, state, player):\n",
    "        \"\"\"\n",
    "        Returns the AI's chosen action based on its strategy.\n",
    "        \"\"\"\n",
    "        infoset = state.information_state_string(player)\n",
    "        legal_actions = state.legal_actions(player)\n",
    "        strategy = self.get_average_strategy_for_player(player).get(infoset)\n",
    "\n",
    "        if strategy is not None:\n",
    "            # Extract probabilities for legal actions\n",
    "            action_probs = np.array([strategy[action] for action in legal_actions])\n",
    "            # Normalize the probabilities\n",
    "            total_prob = action_probs.sum()\n",
    "            if total_prob > 0:\n",
    "                action_probs /= total_prob\n",
    "            else:\n",
    "                action_probs = np.ones(len(legal_actions)) / len(legal_actions)\n",
    "        else:\n",
    "            # Default to uniform probabilities if no strategy is available\n",
    "            action_probs = np.ones(len(legal_actions)) / len(legal_actions)\n",
    "\n",
    "        # Choose action according to the probabilities\n",
    "        action_idx = np.random.choice(len(legal_actions), p=action_probs)\n",
    "        action = legal_actions[action_idx]\n",
    "        return action\n",
    "\n",
    "    def get_average_strategy_for_player(self, player):\n",
    "        \"\"\"\n",
    "        Returns the average strategy dictionary for the specified player.\n",
    "        \"\"\"\n",
    "        if player == 0:\n",
    "            return self.average_strategy_p0\n",
    "        else:\n",
    "            return self.average_strategy_p1\n",
    "\n",
    "    def display_state(self, state, player):\n",
    "        # Display coins and known information\n",
    "        print(f\"Your coins: {state.coins[player]}\")\n",
    "        print(f\"AI's coins: {state.coins[1 - player]}\")\n",
    "\n",
    "        # Display your cards (since you know them)\n",
    "        print(f\"Your cards: {state.cards[player]}\")\n",
    "\n",
    "        # For simplicity, we won't display the AI's cards or any hidden information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c03c04",
   "metadata": {},
   "source": [
    "# MCCFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65772d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MCCFR:\n",
    "    def __init__(self, game, iterations=10000):\n",
    "        self.game = game\n",
    "        self.iterations = iterations\n",
    "\n",
    "        self.regret_sums = defaultdict(lambda: np.zeros(game.num_distinct_actions()))\n",
    "        self.strategy_sums = defaultdict(lambda: np.zeros(game.num_distinct_actions()))\n",
    "        self.node_visits = defaultdict(float)\n",
    "\n",
    "    def regret_matching(self, regrets):\n",
    "        positive_regrets = np.maximum(regrets, 0)\n",
    "        sum_positive = positive_regrets.sum()\n",
    "        if sum_positive > 0:\n",
    "            return positive_regrets / sum_positive\n",
    "        else:\n",
    "            num_actions = len(regrets)\n",
    "            return np.ones(num_actions) / num_actions\n",
    "\n",
    "    def cfr(self, state, player):\n",
    "        if state.is_terminal():\n",
    "            return state.returns()[player]\n",
    "\n",
    "        if state.is_chance_node():\n",
    "            outcomes = state.chance_outcomes()\n",
    "            actions, probs = zip(*outcomes)\n",
    "            action_idx = np.random.choice(len(actions), p=probs)\n",
    "            action = actions[action_idx]\n",
    "            state.apply_action(action)\n",
    "            return self.cfr(state, player)\n",
    "\n",
    "        current_player = state.current_player()\n",
    "        infoset = state.information_state_string(current_player)\n",
    "        legal_actions = state.legal_actions(current_player)\n",
    "        num_actions = len(legal_actions)\n",
    "        legal_action_indices = np.array(legal_actions)\n",
    "        key = (current_player, infoset)\n",
    "\n",
    "        if current_player == player:\n",
    "            # Get the regrets for legal actions\n",
    "            regrets = self.regret_sums[key][legal_action_indices]\n",
    "            strategy = self.regret_matching(regrets)\n",
    "\n",
    "            # Expand strategy to full action vector\n",
    "            full_strategy = np.zeros(self.game.num_distinct_actions())\n",
    "            full_strategy[legal_action_indices] = strategy\n",
    "\n",
    "            # Update strategy sums and node visits\n",
    "            self.strategy_sums[key] += full_strategy\n",
    "            self.node_visits[key] += 1\n",
    "\n",
    "            util = np.zeros(num_actions)\n",
    "            node_util = 0.0\n",
    "            for idx, action in enumerate(legal_actions):\n",
    "                next_state = state.clone()\n",
    "                next_state.apply_action(action)\n",
    "                util[idx] = self.cfr(next_state, player)\n",
    "                node_util += strategy[idx] * util[idx]\n",
    "            regrets = util - node_util\n",
    "            # Update regrets for legal actions\n",
    "            self.regret_sums[key][legal_action_indices] += regrets\n",
    "            return node_util\n",
    "        else:\n",
    "            # Sample opponent's action according to a fixed uniform random strategy\n",
    "            opponent_strategy = np.ones(num_actions) / num_actions\n",
    "            action_idx = np.random.choice(num_actions, p=opponent_strategy)\n",
    "            action = legal_actions[action_idx]\n",
    "            state.apply_action(action)\n",
    "            return self.cfr(state, player)\n",
    "\n",
    "    def train(self, verbose=False):\n",
    "        switch_tqdm = not verbose\n",
    "        for _ in tqdm(range(self.iterations), disable=switch_tqdm):\n",
    "            for player in range(self.game.num_players()):\n",
    "                state = self.game.new_initial_state()\n",
    "                self.cfr(state, player)\n",
    "\n",
    "    def get_average_strategy(self):\n",
    "        average_strategy_p0 = {}\n",
    "        average_strategy_p1 = {}\n",
    "\n",
    "        for key in self.strategy_sums:\n",
    "            player, infoset = key\n",
    "            strategy_sum = self.strategy_sums[key]\n",
    "            visits = self.node_visits[key]\n",
    "            if visits > 0:\n",
    "                average_strategy = strategy_sum / visits\n",
    "                total = average_strategy.sum()\n",
    "                if total > 0:\n",
    "                    average_strategy /= total\n",
    "                else:\n",
    "                    average_strategy = np.ones(len(average_strategy)) / len(average_strategy)\n",
    "            else:\n",
    "                num_actions = len(strategy_sum)\n",
    "                average_strategy = np.ones(num_actions) / num_actions\n",
    "\n",
    "            if player == 0:\n",
    "                average_strategy_p0[infoset] = average_strategy\n",
    "            else:\n",
    "                average_strategy_p1[infoset] = average_strategy\n",
    "\n",
    "        return average_strategy_p0, average_strategy_p1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca597b1d",
   "metadata": {},
   "source": [
    "## Selfplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_self_play(game, average_strategy):\n",
    "    state = game.new_initial_state()\n",
    "    while not state.is_terminal():\n",
    "        current_player = state.current_player()\n",
    "        if state.is_chance_node():\n",
    "            # Handle chance node\n",
    "            outcomes = state.chance_outcomes()\n",
    "            actions, probs = zip(*outcomes)\n",
    "            action = np.random.choice(actions, p=probs)\n",
    "            state.apply_action(action)\n",
    "        else:\n",
    "            infoset = state.information_state_string(current_player)\n",
    "            legal_actions = state.legal_actions(current_player)\n",
    "            num_actions = len(legal_actions)\n",
    "            legal_action_indices = np.array(legal_actions)\n",
    "\n",
    "            # Retrieve strategy for the current infoset\n",
    "            strategy_full = average_strategy.get(infoset, np.ones(game.num_distinct_actions()) / game.num_distinct_actions())\n",
    "            # Extract probabilities for legal actions\n",
    "            strategy = strategy_full[legal_action_indices]\n",
    "\n",
    "            # Normalize strategy\n",
    "            strategy_sum = strategy.sum()\n",
    "            if strategy_sum > 0:\n",
    "                strategy /= strategy_sum\n",
    "            else:\n",
    "                strategy = np.ones(num_actions) / num_actions\n",
    "\n",
    "            # Choose action according to strategy\n",
    "            action_idx = np.random.choice(num_actions, p=strategy)\n",
    "            action = legal_actions[action_idx]\n",
    "            state.apply_action(action)\n",
    "\n",
    "    returns = state.returns()\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winrates(solver, game, num_games=100):\n",
    "    \n",
    "    average_strategy = solver.get_average_strategy()\n",
    "    wins = 0\n",
    "    for _ in range(num_games):\n",
    "        returns = simulate_self_play(game, average_strategy)\n",
    "        if returns[0] > returns[1]:\n",
    "            wins += 1  # Player 0 wins\n",
    "    \n",
    "    return wins / num_games\n",
    "\n",
    "\n",
    "#get_winrates(solver, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4ca9c",
   "metadata": {},
   "source": [
    "### best response given a strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b24cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning\n",
    "\n",
    "def compute_best_response(game, opponent_strategy, best_response_player=0, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Computes the best response value for the best_response_player against the opponent's strategy,\n",
    "    using pruning with a specified threshold.\n",
    "\n",
    "    Args:\n",
    "        game: The game object.\n",
    "        opponent_strategy: A dictionary mapping information states to numpy arrays of action probabilities.\n",
    "        best_response_player: The player index (0 or 1) for whom to compute the best response.\n",
    "        threshold: The probability threshold below which branches are pruned.\n",
    "\n",
    "    Returns:\n",
    "        The expected utility (win rate) for the best_response_player.\n",
    "    \"\"\"\n",
    "    print(\"Pruned, Computing BR for: player\", best_response_player)\n",
    "\n",
    "    root_state = game.new_initial_state()\n",
    "    cache = {}\n",
    "\n",
    "    def best_response_value(state):\n",
    "        # Check for terminal state\n",
    "        if state.is_terminal():\n",
    "            returns = state.returns()\n",
    "            return returns[best_response_player]\n",
    "\n",
    "        # Check for chance node\n",
    "        elif state.is_chance_node():\n",
    "            total_util = 0.0\n",
    "            outcomes = state.chance_outcomes()\n",
    "            for action, prob in outcomes:\n",
    "                if prob < threshold:\n",
    "                    continue  # Prune chance outcomes with low probability\n",
    "                next_state = state.clone()\n",
    "                next_state.apply_action(action)\n",
    "                util = best_response_value(next_state)\n",
    "                total_util += prob * util\n",
    "            return total_util\n",
    "\n",
    "        else:\n",
    "            current_player = state.current_player()\n",
    "            infoset = state.information_state_string(current_player)\n",
    "            legal_actions = state.legal_actions()\n",
    "\n",
    "            # Use a unique key for caching to prevent infinite loops\n",
    "            key = (current_player, infoset, tuple(state.history()))\n",
    "\n",
    "            # Check cache to avoid redundant computations\n",
    "            if key in cache:\n",
    "                return cache[key]\n",
    "\n",
    "            # If it's the opponent's turn\n",
    "            if current_player != best_response_player:\n",
    "                strategy = opponent_strategy.get(infoset, None)\n",
    "                if strategy is None:\n",
    "                    # If no strategy is provided, assume uniform random\n",
    "                    num_actions = len(legal_actions)\n",
    "                    strategy = np.ones(num_actions) / num_actions\n",
    "                else:\n",
    "                    # Ensure the strategy corresponds to the legal actions\n",
    "                    strategy = strategy[legal_actions]\n",
    "\n",
    "                    strategy_sum = strategy.sum()\n",
    "                    if strategy_sum > 0:\n",
    "                        strategy /= strategy_sum\n",
    "                    else:\n",
    "                        # If the sum is zero, use a uniform random strategy over legal actions\n",
    "                        num_actions = len(legal_actions)\n",
    "                        strategy = np.ones(num_actions) / num_actions\n",
    "                        #print(f\"Warning: Strategy sum is zero for infoset '{infoset}'. Using uniform strategy.\")\n",
    "\n",
    "                total_util = 0.0\n",
    "                for idx, action in enumerate(legal_actions):\n",
    "                    prob = strategy[idx]\n",
    "                    if prob < threshold:\n",
    "                        continue  # Prune actions with probability less than threshold\n",
    "                    next_state = state.clone()\n",
    "                    next_state.apply_action(action)\n",
    "                    util = best_response_value(next_state)\n",
    "                    total_util += prob * util\n",
    "                cache[key] = total_util  # Store result in cache\n",
    "                return total_util\n",
    "\n",
    "            # If it's the best response player's turn\n",
    "            else:\n",
    "                max_util = float('-inf')\n",
    "                best_action_util = {}\n",
    "                for action in legal_actions:\n",
    "                    next_state = state.clone()\n",
    "                    next_state.apply_action(action)\n",
    "                    util = best_response_value(next_state)\n",
    "                    best_action_util[action] = util\n",
    "                    if util > max_util:\n",
    "                        max_util = util\n",
    "\n",
    "                # Prune actions that are worse than the max utility minus the threshold\n",
    "                pruned_actions = [action for action, util in best_action_util.items()\n",
    "                                  if max_util - util > threshold]\n",
    "\n",
    "                # If all actions are pruned, select the best action\n",
    "                if len(pruned_actions) == len(legal_actions):\n",
    "                    cache[key] = max_util  # Store result in cache\n",
    "                    return max_util\n",
    "                else:\n",
    "                    # Recompute max_util considering pruned actions\n",
    "                    for action in legal_actions:\n",
    "                        if action in pruned_actions:\n",
    "                            continue\n",
    "                        util = best_action_util[action]\n",
    "                        if util > max_util:\n",
    "                            max_util = util\n",
    "                    cache[key] = max_util  # Store result in cache\n",
    "                    return max_util\n",
    "\n",
    "    # Start traversal from the root state\n",
    "    expected_utility = best_response_value(root_state)\n",
    "    return expected_utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4850524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_strategy_vs_strategy(game, strategy_player0, strategy_player1, num_games=1000):\n",
    "    wins = [0, 0]  # Index 0 for Player 0 wins, Index 1 for Player 1 wins\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        state = game.new_initial_state()\n",
    "        while not state.is_terminal():\n",
    "            current_player = state.current_player()\n",
    "            if state.is_chance_node():\n",
    "                # Handle chance node\n",
    "                outcomes = state.chance_outcomes()\n",
    "                actions, probs = zip(*outcomes)\n",
    "                action = np.random.choice(actions, p=probs)\n",
    "                state.apply_action(action)\n",
    "            else:\n",
    "                infoset = state.information_state_string(current_player)\n",
    "                legal_actions = state.legal_actions(current_player)\n",
    "                num_actions = len(legal_actions)\n",
    "                legal_action_indices = np.array(legal_actions)\n",
    "\n",
    "                # Select the appropriate strategy for the current player\n",
    "                if current_player == 0:\n",
    "                    strategy_full = strategy_player0.get(infoset)\n",
    "                else:\n",
    "                    strategy_full = strategy_player1.get(infoset)\n",
    "\n",
    "                if strategy_full is None:\n",
    "                    # Default to uniform random strategy if infoset not found\n",
    "                    num_actions_total = game.num_distinct_actions()\n",
    "                    strategy_full = np.ones(num_actions_total) / num_actions_total\n",
    "\n",
    "                # Extract probabilities for legal actions\n",
    "                strategy = []\n",
    "                for action in legal_actions:\n",
    "                    strategy.append(strategy_full[action])\n",
    "                strategy = np.array(strategy)\n",
    "\n",
    "                # Normalize strategy\n",
    "                strategy_sum = strategy.sum()\n",
    "                if strategy_sum > 0:\n",
    "                    strategy /= strategy_sum\n",
    "                else:\n",
    "                    strategy = np.ones(num_actions) / num_actions\n",
    "\n",
    "                # Choose action according to strategy\n",
    "                action_idx = np.random.choice(num_actions, p=strategy)\n",
    "                action = legal_actions[action_idx]\n",
    "                state.apply_action(action)\n",
    "\n",
    "        returns = state.returns()\n",
    "        \n",
    "        if returns[0] > returns[1]:\n",
    "            wins[0] += 1  # Player 0 wins\n",
    "\n",
    "    # Calculate win rates\n",
    "    win_rate_player0 = wins[0] / num_games\n",
    "\n",
    "    return win_rate_player0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e37b0",
   "metadata": {},
   "source": [
    "# training results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498a9e8",
   "metadata": {},
   "source": [
    "### nash gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f26b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = 10\n",
    "\n",
    "N = 1000\n",
    "Episode = 20\n",
    "\n",
    "\n",
    "N_simulate = 1000 # number of games to evaluate the strategy\n",
    "\n",
    "game = CoupGame(max_action_count=max_count, favorite_pl=0)\n",
    "solver = MCCFR(game, iterations = N)\n",
    "\n",
    "edge_0 = [0] * Episode\n",
    "edge_1 = [0] * Episode\n",
    "\n",
    "for epi in tqdm(range(Episode)):\n",
    "    \n",
    "    solver.train(verbose=True)\n",
    "    \n",
    "    average_strategy_p0, average_strategy_p1 = solver.get_average_strategy()\n",
    "\n",
    "    br_p0 = compute_best_response(game, average_strategy_p1, player=0)\n",
    "    br_p1 = compute_best_response(game, average_strategy_p0, player=1)\n",
    "\n",
    "\n",
    "print(\"COMPLETE!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4313f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, Episode + 1), edge_0, label='P0 is BR')\n",
    "plt.plot(range(1, Episode + 1), edge_1, label='P1 is BR')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.title('Win Rate Over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.1)  # Set y-axis limits from 0 to 1\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))  # Format y-axis as percentages\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dfdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10f41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b777ae",
   "metadata": {},
   "source": [
    "# simulate and play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d8917",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playground = CoupGamePlayground(game, solver,show_ai_suggestion=True)\n",
    "playground.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19657ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12ffef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4bdca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8d56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303d712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89944527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a6b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a9d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628e6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476af84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31184082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f117da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940e218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86579d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a621c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
